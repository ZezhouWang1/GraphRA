{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch_geometric.utils as utils\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1\n",
    "DATASET_NAME = 'PubMed'\n",
    "pt_file_path = '/Users/wangzezhou/Desktop/4880作业2/Code/Data/PubMed/simteg_roberta_x.pt'\n",
    "\n",
    "def load_dataset():\n",
    "    embeddings = torch.load(pt_file_path)\n",
    "    dataset = Planetoid(root='/tmp/PubMed', name=DATASET_NAME)\n",
    "    data = dataset[0]\n",
    "    return embeddings, data\n",
    "\n",
    "def compute_pagerank(data):\n",
    "    nx_graph = to_networkx(data, to_undirected=True)\n",
    "    pagerank_values = nx.pagerank(nx_graph)\n",
    "    return pagerank_values\n",
    "\n",
    "def get_pagerank(pagerank_values, neighbours):\n",
    "    values = [pagerank_values[neighbour] for neighbour in neighbours]\n",
    "    return torch.tensor(values)\n",
    "\n",
    "def compute_similarity(embeddings, node, neighbors):\n",
    "\n",
    "    node_embedding = embeddings[node].unsqueeze(0)  # 将 node_embedding 转换为二维张量\n",
    "    neighbor_embeddings = embeddings[neighbors]\n",
    "\n",
    "    node_embedding = node_embedding / node_embedding.norm(dim=1, keepdim=True)\n",
    "    neighbor_embeddings = neighbor_embeddings / neighbor_embeddings.norm(dim=1, keepdim=True)\n",
    "\n",
    "    sim_scores = torch.mm(node_embedding, neighbor_embeddings.t()).squeeze(0)\n",
    "    return sim_scores\n",
    "\n",
    "def get_2hop_neighbors(data):\n",
    "    # data.edge_index = to_undirected(data.edge_index)\n",
    "    set_idx = torch.arange(data.num_nodes)  # all nodes\n",
    "    loader = NeighborLoader(data, input_nodes=set_idx, num_neighbors=[-1, -1], batch_size=1, shuffle=False)\n",
    "\n",
    "    two_hop_neighbors = {}\n",
    "    for batch in tqdm(loader, desc=\"Processing nodes\"):\n",
    "        if batch.edge_index.size(1) == 0:\n",
    "            continue\n",
    "\n",
    "        node = batch.input_id.item()\n",
    "        neighbors = batch.n_id.tolist()\n",
    "\n",
    "        if node in neighbors:\n",
    "            neighbors.remove(node)\n",
    "        two_hop_neighbors[node] = neighbors\n",
    "\n",
    "    return two_hop_neighbors\n",
    "\n",
    "def find_top_k_neighbors(embeddings, two_hop_neighbors, k):\n",
    "    top_k_neighbors = {}\n",
    "    for node, neighbors in tqdm(two_hop_neighbors.items(), desc=\"Finding top-k neighbors\"):\n",
    "        if len(neighbors) >= k:\n",
    "            neighbor_sims = compute_similarity(embeddings, node, neighbors)\n",
    "            top_k = torch.topk(neighbor_sims, k=k, largest=True)\n",
    "            top_k_neighbors[node] = [neighbors[i] for i in top_k.indices.tolist()]\n",
    "        elif len(neighbors) > 0:\n",
    "            neighbor_sims = compute_similarity(embeddings, node, neighbors)\n",
    "            top_k = torch.topk(neighbor_sims, k=len(neighbors), largest=True)\n",
    "            sorted_neighbors = [neighbors[i] for i in top_k.indices.tolist()]\n",
    "\n",
    "            repeats_per_neighbor = math.ceil(k / len(sorted_neighbors))\n",
    "            extended_neighbors = []\n",
    "            for neighbor in sorted_neighbors:\n",
    "                extended_neighbors.extend([neighbor] * repeats_per_neighbor)\n",
    "\n",
    "            top_k_neighbors[node] = extended_neighbors[:k]\n",
    "        else:\n",
    "            top_k_neighbors[node] = [node] * k\n",
    "\n",
    "    return top_k_neighbors\n",
    "\n",
    "def prepare_for_json(data):\n",
    "    if isinstance(data, dict):\n",
    "        new_dict = {}\n",
    "        for key, value in data.items():\n",
    "            if isinstance(key, torch.Tensor):\n",
    "                new_key = key.item() if key.numel() == 1 else key.tolist()\n",
    "            else:\n",
    "                new_key = str(key)\n",
    "            new_dict[new_key] = prepare_for_json(value)\n",
    "        return new_dict\n",
    "    elif isinstance(data, list):\n",
    "        return [prepare_for_json(element) for element in data]\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        return data.tolist()\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def save_to_json(data, file_name):\n",
    "    data = prepare_for_json(data)\n",
    "    print(\"save begin\")\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "def save_to_pt(data, file_name):\n",
    "    torch.save(data, file_name)\n",
    "\n",
    "def extract():\n",
    "    embeddings, data = load_dataset()\n",
    "    two_hop_neighbors = get_2hop_neighbors(data)\n",
    "    k = K  \n",
    "    top_k_neighbors = find_top_k_neighbors(embeddings, two_hop_neighbors, k)\n",
    "    save_to_json(top_k_neighbors, f'./Code/Data/cosine/top_{k}_neighbors.json')\n",
    "\n",
    "def main5():\n",
    "    extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes: 100%|██████████| 19717/19717 [00:06<00:00, 3260.96it/s]\n",
      "Finding top-k neighbors: 100%|██████████| 19717/19717 [00:11<00:00, 1704.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save begin\n"
     ]
    }
   ],
   "source": [
    "main5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1\n",
    "DATASET_NAME = 'PubMed'  # 'ogbn-products'  # 'ogbn-arxiv'\n",
    "pt_file_path = '/Users/wangzezhou/Desktop/4880作业2/Code/Data/PubMed/simteg_roberta_x.pt'\n",
    "\n",
    "\n",
    "def load_dataset(pt_file_path, set):\n",
    "    embeddings = torch.load(pt_file_path)\n",
    "    dataset = Planetoid(root='/tmp/PubMed', name=DATASET_NAME)\n",
    "    data = dataset[0]\n",
    "    \n",
    "    if set == 'train':\n",
    "        set_idx = data.train_mask.nonzero(as_tuple=False).view(-1)\n",
    "    elif set == 'valid':\n",
    "        set_idx = data.val_mask.nonzero(as_tuple=False).view(-1)\n",
    "    elif set == 'test':\n",
    "        set_idx = data.test_mask.nonzero(as_tuple=False).view(-1)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid set name: {set}. Must be one of ['train', 'valid', 'test']\")\n",
    "    \n",
    "    return embeddings, set_idx, data\n",
    "\n",
    "\n",
    "def compute_similarity(embeddings, node, neighbors):\n",
    "\n",
    "    node_embedding = embeddings[node] \n",
    "    neighbor_embeddings = embeddings[neighbors]\n",
    "\n",
    "    node_embedding = node_embedding / node_embedding.norm(dim=1, keepdim=True)\n",
    "    neighbor_embeddings = neighbor_embeddings / neighbor_embeddings.norm(dim=1, keepdim=True)\n",
    "\n",
    "    sim_scores = torch.mm(node_embedding, neighbor_embeddings.t()).squeeze(0)\n",
    "    return sim_scores\n",
    "\n",
    "\n",
    "def get_2hop_neighbors(data, set_idx):\n",
    "    data.edge_index = to_undirected(data.edge_index)\n",
    "    loader = NeighborLoader(data, input_nodes=set_idx, num_neighbors=[-1, -1], batch_size=1, shuffle=False)\n",
    "\n",
    "    two_hop_neighbors = {}\n",
    "    for batch in tqdm(loader, desc=\"Processing nodes\"):\n",
    "        if batch.edge_index.size(1) == 0:\n",
    "            continue  \n",
    "\n",
    "        node = batch.input_id\n",
    "        neighbors = batch.n_id.tolist()\n",
    "\n",
    "        if node in neighbors:\n",
    "            neighbors.remove(node)  \n",
    "        two_hop_neighbors[node] = neighbors\n",
    "\n",
    "    return two_hop_neighbors\n",
    "\n",
    "\n",
    "def find_top_k_neighbors(embeddings, two_hop_neighbors, k):\n",
    "    top_k_neighbors = {}\n",
    "    for node, neighbors in tqdm(two_hop_neighbors.items(), desc=\"Finding top-k neighbors\"):\n",
    "        if len(neighbors) >= k:\n",
    "           \n",
    "            neighbor_sims = compute_similarity(embeddings, node, neighbors)\n",
    "            top_k = torch.topk(neighbor_sims, k=k, largest=True)\n",
    "            top_k_neighbors[node] = [neighbors[i] for i in top_k.indices.tolist()]\n",
    "        elif len(neighbors) > 0:\n",
    "           \n",
    "            neighbor_sims = compute_similarity(embeddings, node, neighbors)\n",
    "            top_k = torch.topk(neighbor_sims, k=len(neighbors), largest=True)\n",
    "            sorted_neighbors = [neighbors[i] for i in top_k.indices.tolist()]\n",
    "\n",
    "            repeats_per_neighbor = math.ceil(k / len(sorted_neighbors))\n",
    "            extended_neighbors = []\n",
    "            for neighbor in sorted_neighbors:\n",
    "                extended_neighbors.extend([neighbor] * repeats_per_neighbor)\n",
    "\n",
    "            top_k_neighbors[node] = extended_neighbors[:k]\n",
    "        else:\n",
    "   \n",
    "            top_k_neighbors[node] = [node] * k\n",
    "            print(top_k_neighbors[node])\n",
    "\n",
    "    return top_k_neighbors\n",
    "\n",
    "\n",
    "def prepare_for_json(data):\n",
    "    if isinstance(data, dict):\n",
    "        new_dict = {}\n",
    "        for key, value in data.items():\n",
    "            if isinstance(key, torch.Tensor):\n",
    "         \n",
    "                new_key = key.item() if key.numel() == 1 else key.tolist()\n",
    "            else:\n",
    "                new_key = str(key)\n",
    "            new_dict[new_key] = prepare_for_json(value)\n",
    "        return new_dict\n",
    "    elif isinstance(data, list):\n",
    "        return [prepare_for_json(element) for element in data]\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        return data.tolist()\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "def save_to_json(data, file_name):\n",
    "    data = prepare_for_json(data)\n",
    "    print(\"save begin\")\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n",
    "def save_to_pt(data, file_name):\n",
    "    torch.save(data, file_name)\n",
    "\n",
    "\n",
    "def extract(set):\n",
    "    embeddings, set_idx, data = load_dataset(pt_file_path, set)\n",
    "    two_hop_neighbors = get_2hop_neighbors(data, set_idx)\n",
    "    #save_to_json(two_hop_neighbors , f'./Code/Data/cora/{set}_neighbors.json')\n",
    "    #save_to_pt(two_hop_neighbors, f'./Code/Data/cora/{set}_neighbors.pt')\n",
    "    \n",
    "    # two_hop_neighbors = torch.load(f'/Data/ogbn-products/all-roberta-large-v1/main/cached_embs/{set}_neighbors.pt')\n",
    "    k = K  \n",
    "    top_k_neighbors = find_top_k_neighbors(embeddings, two_hop_neighbors, k)\n",
    "    save_to_json(top_k_neighbors, f'./Code/Data/cosine/{set}_top_{K}_neighbors.json')\n",
    "    #save_to_pt(top_k_neighbors, f'./Code/Data/cora/{set}_top_3_neighbors.pt')\n",
    "\n",
    "\n",
    "def main1():\n",
    "    extract('train')\n",
    "    extract('valid')\n",
    "    extract('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes: 100%|██████████| 60/60 [00:00<00:00, 3656.97it/s]\n",
      "Finding top-k neighbors: 100%|██████████| 60/60 [00:00<00:00, 1396.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save begin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes: 100%|██████████| 500/500 [00:00<00:00, 1920.29it/s]\n",
      "Finding top-k neighbors: 100%|██████████| 500/500 [00:00<00:00, 1797.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save begin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes: 100%|██████████| 1000/1000 [00:00<00:00, 1897.26it/s]\n",
      "Finding top-k neighbors: 100%|██████████| 1000/1000 [00:00<00:00, 1511.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save begin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
